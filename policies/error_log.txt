libbpf: prog 'mglru_evict_folios': BPF program load failed: Permission denied
libbpf: prog 'mglru_evict_folios': -- BEGIN PROG LOAD LOG --
reg type unsupported for arg#0 function mglru_evict_folios#315
0: R1=ctx(off=0,imm=0) R10=fp0
; void BPF_STRUCT_OPS(mglru_evict_folios, struct cache_ext_eviction_ctx *eviction_ctx,
0: (79) r6 = *(u64 *)(r1 +8)
func 'evict_folios' arg1 has btf_id 2223 type STRUCT 'mem_cgroup'
1: R1=ctx(off=0,imm=0) R6_w=trusted_ptr_mem_cgroup(off=0,imm=0)
1: (79) r7 = *(u64 *)(r1 +0)
func 'evict_folios' arg0 has btf_id 1580 type STRUCT 'cache_ext_eviction_ctx'
2: R1=ctx(off=0,imm=0) R7_w=trusted_ptr_cache_ext_eviction_ctx(off=0,imm=0)
2: (b7) r1 = 0                        ; R1_w=0
; DEFINE_LRUGEN_void;
3: (63) *(u32 *)(r10 -112) = r1       ; R1_w=0 R10=fp0 fp-112=????0000
4: (bf) r2 = r10                      ; R2_w=fp0 R10=fp0
;
5: (07) r2 += -112                    ; R2_w=fp-112
; DEFINE_LRUGEN_void;
6: (18) r1 = 0xffff888109571400       ; R1_w=map_ptr(off=0,ks=4,vs=256,imm=0)
8: (85) call bpf_map_lookup_elem#1    ; R0_w=map_value_or_null(id=1,off=0,ks=4,vs=256,imm=0)
9: (bf) r8 = r0                       ; R0_w=map_value_or_null(id=1,off=0,ks=4,vs=256,imm=0) R8_w=map_value_or_null(id=1,off=0,ks=4,vs=256,imm=0)
10: (55) if r8 != 0x0 goto pc+4 15: R0_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R6_w=trusted_ptr_mem_cgroup(off=0,imm=0) R7_w=trusted_ptr_cache_ext_eviction_ctx(off=0,imm=0) R8_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-112=????mmmm
; DEFINE_LRUGEN_void;
15: (7b) *(u64 *)(r10 -256) = r6      ; R6_w=trusted_ptr_mem_cgroup(off=0,imm=0) R10=fp0 fp-256_w=trusted_ptr_
; bpf_printk("num folios in eviction request: %d", eviction_ctx->request_nr_folios_to_evict);
16: (79) r3 = *(u64 *)(r7 +0)         ; R3_w=scalar() R7_w=trusted_ptr_cache_ext_eviction_ctx(off=0,imm=0)
17: (18) r1 = 0xffffc900001721f5      ; R1_w=map_value(off=501,ks=4,vs=1478,imm=0)
19: (b7) r2 = 35                      ; R2_w=35
20: (85) call bpf_trace_printk#6      ; R0_w=scalar()
; bpf_spin_lock(&lrugen->lock);
21: (bf) r1 = r8                      ; R1_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R8_w=map_value(id=1,off=0,ks=4,vs=256,imm=0)
22: (85) call bpf_spin_lock#93        ;
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
23: (79) r5 = *(u64 *)(r8 +16)        ; R5_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
24: (79) r4 = *(u64 *)(r8 +8)         ; R4_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
25: (79) r3 = *(u64 *)(r8 +16)        ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; if (min_seq + MIN_NR_GENS > max_seq) {
26: (bf) r2 = r3                      ; R2_w=scalar(id=2) R3_w=scalar(id=2)
27: (07) r2 += 2                      ; R2_w=scalar()
28: (bf) r1 = r4                      ; R1_w=scalar(id=3) R4_w=scalar(id=3)
29: (7b) *(u64 *)(r10 -216) = r8      ; R8=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216_w=map_value
; if (min_seq + MIN_NR_GENS > max_seq) {
30: (7b) *(u64 *)(r10 -240) = r7      ; R7=trusted_ptr_cache_ext_eviction_ctx(off=0,imm=0) R10=fp0 fp-240_w=trusted_ptr_
31: (2d) if r2 > r4 goto pc+78        ; R2_w=scalar() R4_w=scalar(id=3)
32: (7b) *(u64 *)(r10 -264) = r5      ; R5_w=scalar() R10=fp0 fp-264_w=mmmmmmmm
33: (7b) *(u64 *)(r10 -248) = r4      ; R4_w=scalar(id=3) R10=fp0 fp-248_w=mmmmmmmm
34: (bf) r5 = r4                      ; R4_w=scalar(id=3) R5_w=scalar(id=3)
; int max_iter =  min(MAX_NR_GENS, max_seq - min_seq);
35: (1f) r5 -= r3                     ; R3_w=scalar(id=2) R5_w=scalar()
36: (b7) r1 = 4                       ; R1_w=4
37: (bf) r6 = r5                      ; R5_w=scalar(id=4) R6_w=scalar(id=4)
38: (2d) if r1 > r5 goto pc+1         ; R1_w=4 R5_w=scalar(id=4,umin=4)
39: (b7) r6 = 4                       ; R6=4
40: (7b) *(u64 *)(r10 -272) = r2      ; R2=scalar() R10=fp0 fp-272_w=mmmmmmmm
41: (b7) r1 = 0                       ; R1_w=0
42: (b7) r7 = 0                       ; R7_w=0
43: (b7) r2 = 0                       ; R2_w=0
; for (int i = 0; i < max_iter; i++) {
44: (7b) *(u64 *)(r10 -224) = r2      ; R2_w=0 R10=fp0 fp-224_w=00000000
45: (15) if r6 == 0x0 goto pc+49      ; R6=4
46: (bf) r1 = r5                      ; R1_w=scalar(id=4,umin=4) R5=scalar(id=4,umin=4)
47: (07) r1 += -2                     ; R1_w=scalar()
48: (7b) *(u64 *)(r10 -232) = r1      ; R1_w=scalar() R10=fp0 fp-232_w=mmmmmmmm
49: (b7) r2 = 0                       ; R2_w=0
50: (b7) r1 = 0                       ; R1_w=0
51: (7b) *(u64 *)(r10 -224) = r1      ; R1_w=0 R10=fp0 fp-224_w=00000000
52: (b7) r7 = 0                       ; R7_w=0
53: (b7) r1 = 0                       ; R1_w=0
54: (05) goto pc+8
; for (int i = 0; i < max_iter; i++) {
63: (bf) r4 = r1                      ; R1=0 R4_w=0
; return seq % MAX_NR_GENS;
64: (bf) r1 = r3                      ; R1_w=scalar(id=2) R3=scalar(id=2)
65: (0f) r1 += r2                     ; R1_w=scalar() R2=0
66: (57) r1 &= 3                      ; R1_w=scalar(umax=3,var_off=(0x0; 0x3))
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
67: (67) r1 <<= 3                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18))
68: (79) r9 = *(u64 *)(r10 -216)      ; R9_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
69: (0f) r9 += r1                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18)) R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
70: (b7) r0 = 0                       ; R0_w=0
71: (db) r0 = atomic64_fetch_or((u64 *)(r9 +224), r0)         ; R0_w=scalar() R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
72: (b7) r1 = 0                       ; R1_w=0
73: (b7) r8 = 0                       ; R8_w=0
74: (6d) if r8 s> r0 goto pc-20       ; R0_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8_w=0
75: (07) r9 += 224                    ; R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
76: (b7) r1 = 0                       ; R1_w=0
77: (b7) r0 = 0                       ; R0_w=0
78: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
; size += max(read_nr_pages_stat(lrugen, gen), 0L);
79: (b7) r8 = 1                       ; R8_w=1
80: (6d) if r8 s> r0 goto pc-26       ; R0_w=scalar(umin=1,umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8_w=1
81: (b7) r1 = 0                       ; R1_w=0
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
82: (b7) r0 = 0                       ; R0_w=0
83: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0=scalar() R9=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
84: (6d) if r1 s> r0 goto pc-30       ; R0=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R1=0
85: (b7) r1 = 0                       ; R1_w=0
86: (db) r1 = atomic64_fetch_or((u64 *)(r9 +0), r1)   ; R1_w=scalar() R9=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
87: (05) goto pc-33
; if (seq == max_seq)
55: (5d) if r5 != r2 goto pc+32       ; R2=0 R5=scalar(id=4,umin=4)
; else if (seq + MIN_NR_GENS == max_seq)
88: (79) r0 = *(u64 *)(r10 -232)      ; R0_w=scalar() R10=fp0 fp-232=mmmmmmmm
89: (5d) if r0 != r2 goto pc-31       ; R0_w=0 R2=0
; old += size;
90: (bf) r0 = r1                      ; R0_w=scalar(id=5) R1_w=scalar(id=5)
91: (79) r9 = *(u64 *)(r10 -224)      ; R9_w=P0 R10=fp0 fp-224=00000000
92: (0f) r0 += r9                     ; R0_w=scalar() R9_w=P0
93: (7b) *(u64 *)(r10 -224) = r0      ; R0_w=scalar() R10=fp0 fp-224_w=mmmmmmmm
94: (05) goto pc-36
;
59: (0f) r1 += r4                     ; R1_w=scalar() R4=0
; for (int i = 0; i < max_iter; i++) {
60: (07) r2 += 1                      ; R2_w=1
; for (int i = 0; i < max_iter; i++) {
61: (2d) if r6 > r2 goto pc+1         ; R2_w=1 R6=4
63: (bf) r4 = r1                      ; R1_w=scalar(id=6) R4_w=scalar(id=6)
; return seq % MAX_NR_GENS;
64: (bf) r1 = r3                      ; R1_w=scalar(id=2) R3=scalar(id=2)
65: (0f) r1 += r2                     ; R1_w=scalar() R2_w=1
66: (57) r1 &= 3                      ; R1_w=scalar(umax=3,var_off=(0x0; 0x3))
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
67: (67) r1 <<= 3                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18))
68: (79) r9 = *(u64 *)(r10 -216)      ; R9_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
69: (0f) r9 += r1                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18)) R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
70: (b7) r0 = 0                       ; R0_w=0
71: (db) r0 = atomic64_fetch_or((u64 *)(r9 +224), r0)         ; R0_w=scalar() R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
72: (b7) r1 = 0                       ; R1_w=0
73: (b7) r8 = 0                       ; R8_w=0
74: (6d) if r8 s> r0 goto pc-20       ; R0_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8_w=0
75: (07) r9 += 224                    ; R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
76: (b7) r1 = 0                       ; R1_w=0
77: (b7) r0 = 0                       ; R0_w=0
78: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
; size += max(read_nr_pages_stat(lrugen, gen), 0L);
79: (b7) r8 = 1                       ; R8=1
80: (6d) if r8 s> r0 goto pc-26       ; R0=scalar(umin=1,umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=1
81: (b7) r1 = 0                       ; R1_w=0
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
82: (b7) r0 = 0                       ; R0_w=0
83: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
84: (6d) if r1 s> r0 goto pc-30       ; R0_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R1_w=0
85: (b7) r1 = 0                       ; R1_w=0
86: (db) r1 = atomic64_fetch_or((u64 *)(r9 +0), r1)   ; R1_w=scalar() R9=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
87: (05) goto pc-33
; if (seq == max_seq)
55: (5d) if r5 != r2 goto pc+32       ; R2=1 R5=scalar(id=4,umin=4)
; else if (seq + MIN_NR_GENS == max_seq)
88: (79) r0 = *(u64 *)(r10 -232)      ; R0_w=scalar() R10=fp0 fp-232=mmmmmmmm
89: (5d) if r0 != r2 goto pc-31       ; R0_w=1 R2=1
; old += size;
90: (bf) r0 = r1                      ; R0_w=scalar(id=7) R1=scalar(id=7)
91: (79) r9 = *(u64 *)(r10 -224)      ; R9_w=scalar() R10=fp0 fp-224=mmmmmmmm
92: (0f) r0 += r9                     ; R0_w=scalar() R9_w=scalar()
93: (7b) *(u64 *)(r10 -224) = r0      ; R0_w=scalar() R10=fp0 fp-224_w=mmmmmmmm
94: (05) goto pc-36
;
59: (0f) r1 += r4                     ; R1_w=scalar() R4=scalar(id=6)
; for (int i = 0; i < max_iter; i++) {
60: (07) r2 += 1                      ; R2=2
; for (int i = 0; i < max_iter; i++) {
61: (2d) if r6 > r2 goto pc+1         ; R2=2 R6=4
63: (bf) r4 = r1                      ; R1=scalar(id=8) R4_w=scalar(id=8)
; return seq % MAX_NR_GENS;
64: (bf) r1 = r3                      ; R1_w=scalar(id=2) R3=scalar(id=2)
65: (0f) r1 += r2                     ; R1_w=scalar() R2=2
66: (57) r1 &= 3                      ; R1_w=scalar(umax=3,var_off=(0x0; 0x3))
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
67: (67) r1 <<= 3                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18))
68: (79) r9 = *(u64 *)(r10 -216)      ; R9_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
69: (0f) r9 += r1                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18)) R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
70: (b7) r0 = 0                       ; R0_w=0
71: (db) r0 = atomic64_fetch_or((u64 *)(r9 +224), r0)         ; R0_w=scalar() R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
72: (b7) r1 = 0                       ; R1_w=0
73: (b7) r8 = 0                       ; R8_w=0
74: (6d) if r8 s> r0 goto pc-20       ; R0_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8_w=0
75: (07) r9 += 224                    ; R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
76: (b7) r1 = 0                       ; R1_w=0
77: (b7) r0 = 0                       ; R0_w=0
78: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
; size += max(read_nr_pages_stat(lrugen, gen), 0L);
79: (b7) r8 = 1                       ; R8_w=1
80: (6d) if r8 s> r0 goto pc-26       ; R0_w=scalar(umin=1,umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8_w=1
81: (b7) r1 = 0                       ; R1_w=0
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
82: (b7) r0 = 0                       ; R0_w=0
83: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
84: (6d) if r1 s> r0 goto pc-30       ; R0_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R1_w=0
85: (b7) r1 = 0                       ; R1_w=0
86: (db) r1 = atomic64_fetch_or((u64 *)(r9 +0), r1)   ; R1_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
87: (05) goto pc-33
; if (seq == max_seq)
55: (5d) if r5 != r2 goto pc+32       ; R2=2 R5=scalar(id=4,umin=4)
; else if (seq + MIN_NR_GENS == max_seq)
88: (79) r0 = *(u64 *)(r10 -232)      ; R0_w=scalar() R10=fp0 fp-232=mmmmmmmm
89: (5d) if r0 != r2 goto pc-31       ; R0_w=2 R2=2
; old += size;
90: (bf) r0 = r1                      ; R0_w=scalar(id=9) R1=scalar(id=9)
91: (79) r9 = *(u64 *)(r10 -224)      ; R9_w=scalar() R10=fp0 fp-224=mmmmmmmm
92: (0f) r0 += r9                     ; R0_w=scalar() R9_w=scalar()
93: (7b) *(u64 *)(r10 -224) = r0      ; R0_w=scalar() R10=fp0 fp-224_w=mmmmmmmm
94: (05) goto pc-36
;
59: (0f) r1 += r4                     ; R1_w=scalar() R4=scalar(id=8)
; for (int i = 0; i < max_iter; i++) {
60: (07) r2 += 1                      ; R2_w=3
; for (int i = 0; i < max_iter; i++) {
61: (2d) if r6 > r2 goto pc+1         ; R2_w=3 R6=4
63: (bf) r4 = r1                      ; R1_w=scalar(id=10) R4_w=scalar(id=10)
; return seq % MAX_NR_GENS;
64: (bf) r1 = r3                      ; R1_w=scalar(id=2) R3=scalar(id=2)
65: (0f) r1 += r2                     ; R1_w=scalar() R2_w=3
66: (57) r1 &= 3                      ; R1_w=scalar(umax=3,var_off=(0x0; 0x3))
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
67: (67) r1 <<= 3                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18))
68: (79) r9 = *(u64 *)(r10 -216)      ; R9_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
69: (0f) r9 += r1                     ; R1_w=scalar(umax=24,var_off=(0x0; 0x18)) R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
70: (b7) r0 = 0                       ; R0_w=0
71: (db) r0 = atomic64_fetch_or((u64 *)(r9 +224), r0)         ; R0_w=scalar() R9_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
72: (b7) r1 = 0                       ; R1_w=0
73: (b7) r8 = 0                       ; R8=0
74: (6d) if r8 s> r0 goto pc-20       ; R0=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=0
75: (07) r9 += 224                    ; R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
76: (b7) r1 = 0                       ; R1_w=0
77: (b7) r0 = 0                       ; R0_w=0
78: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
; size += max(read_nr_pages_stat(lrugen, gen), 0L);
79: (b7) r8 = 1                       ; R8_w=1
80: (6d) if r8 s> r0 goto pc-26       ; R0_w=scalar(umin=1,umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8_w=1
81: (b7) r1 = 0                       ; R1_w=0
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
82: (b7) r0 = 0                       ; R0_w=0
83: (db) r0 = atomic64_fetch_or((u64 *)(r9 +0), r0)   ; R0_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
84: (6d) if r1 s> r0 goto pc-30       ; R0_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R1_w=0
85: (b7) r1 = 0                       ; R1_w=0
86: (db) r1 = atomic64_fetch_or((u64 *)(r9 +0), r1)   ; R1_w=scalar() R9_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
87: (05) goto pc-33
; if (seq == max_seq)
55: (5d) if r5 != r2 goto pc+32       ; R2=3 R5=scalar(id=4,umin=4)
; else if (seq + MIN_NR_GENS == max_seq)
88: (79) r0 = *(u64 *)(r10 -232)      ; R0=scalar() R10=fp0 fp-232=mmmmmmmm
89: (5d) if r0 != r2 goto pc-31       ; R0=3 R2=3
; old += size;
90: (bf) r0 = r1                      ; R0_w=scalar(id=11) R1=scalar(id=11)
91: (79) r9 = *(u64 *)(r10 -224)      ; R9_w=scalar() R10=fp0 fp-224=mmmmmmmm
92: (0f) r0 += r9                     ; R0_w=scalar() R9_w=scalar()
93: (7b) *(u64 *)(r10 -224) = r0      ; R0_w=scalar() R10=fp0 fp-224_w=mmmmmmmm
94: (05) goto pc-36
;
59: (0f) r1 += r4                     ; R1_w=scalar() R4=scalar(id=10)
; for (int i = 0; i < max_iter; i++) {
60: (07) r2 += 1                      ; R2_w=4
; for (int i = 0; i < max_iter; i++) {
61: (2d) if r6 > r2 goto pc+1         ; R2_w=4 R6=4
62: (05) goto pc+32
; old += size;
95: (b7) r6 = 0                       ; R6_w=0
96: (79) r2 = *(u64 *)(r10 -240)      ; R2_w=trusted_ptr_cache_ext_eviction_ctx(off=0,imm=0) R10=fp0 fp-240=trusted_ptr_
97: (79) r8 = *(u64 *)(r10 -216)      ; R8_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
98: (79) r4 = *(u64 *)(r10 -248)      ; R4_w=scalar() R10=fp0 fp-248=mmmmmmmm
99: (79) r5 = *(u64 *)(r10 -264)      ; R5_w=scalar() R10=fp0 fp-264=mmmmmmmm
; if (min_seq + MIN_NR_GENS < max_seq)
100: (79) r2 = *(u64 *)(r10 -272)     ; R2_w=scalar() R10=fp0 fp-272=mmmmmmmm
101: (2d) if r4 > r2 goto pc+109      ; R2_w=scalar() R4_w=scalar()
; if (young * MIN_NR_GENS > total)
102: (67) r7 <<= 1                    ; R7_w=0
; if (young * MIN_NR_GENS > total)
103: (2d) if r7 > r1 goto pc+3        ; R1=scalar() R7_w=0
104: (79) r2 = *(u64 *)(r10 -224)     ; R2_w=scalar() R10=fp0 fp-224=mmmmmmmm
105: (67) r2 <<= 2                    ; R2=scalar(smax=9223372036854775804,umax=18446744073709551612,var_off=(0x0; 0xfffffffffffffffc),s32_max=2147483644,u32_max=-4)
106: (3d) if r2 >= r1 goto pc+104     ; R1=scalar() R2=scalar(smax=9223372036854775804,umax=18446744073709551612,var_off=(0x0; 0xfffffffffffffffc),s32_max=2147483644,u32_max=-4)
; return lrugen->max_seq - lrugen->min_seq + 1;
107: (79) r3 = *(u64 *)(r8 +16)       ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; return lrugen->max_seq - lrugen->min_seq + 1;
108: (79) r1 = *(u64 *)(r8 +8)        ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
109: (79) r7 = *(u64 *)(r10 -240)     ; R7_w=trusted_ptr_cache_ext_eviction_ctx(off=0,imm=0) R10=fp0 fp-240=trusted_ptr_
; return lrugen->max_seq - lrugen->min_seq + 1;
110: (bf) r2 = r1                     ; R1_w=scalar(id=12) R2_w=scalar(id=12)
111: (1f) r2 -= r3                    ; R2_w=scalar() R3_w=scalar()
; return lrugen->max_seq - lrugen->min_seq + 1;
112: (67) r2 <<= 32                   ; R2_w=scalar(smax=9223372032559808512,umax=18446744069414584320,var_off=(0x0; 0xffffffff00000000),s32_min=0,s32_max=0,u32_max=0)
113: (77) r2 >>= 32                   ; R2_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff))
; if (get_nr_gens(lrugen) == MAX_NR_GENS) {
114: (55) if r2 != 0x3 goto pc+93     ; R2_w=3
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
115: (79) r2 = *(u64 *)(r8 +16)       ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; return seq % MAX_NR_GENS;
116: (57) r2 &= 3                     ; R2_w=scalar(umax=3,var_off=(0x0; 0x3))
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
117: (67) r2 <<= 3                    ; R2_w=scalar(umax=24,var_off=(0x0; 0x18))
118: (bf) r1 = r8                     ; R1_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
119: (0f) r1 += r2                    ; R1_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18)) R2_w=scalar(umax=24,var_off=(0x0; 0x18))
120: (b7) r3 = 0                      ; R3_w=0
121: (b7) r2 = 0                      ; R2_w=0
122: (db) r2 = atomic64_fetch_or((u64 *)(r1 +224), r2)        ; R1=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18)) R2=scalar()
123: (6d) if r3 s> r2 goto pc+7       ; R2=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R3=0
124: (07) r1 += 224                   ; R1_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
125: (b7) r2 = 0                      ; R2_w=0
126: (db) r2 = atomic64_fetch_or((u64 *)(r1 +0), r2)          ; R1_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18)) R2_w=scalar()
127: (b7) r6 = 1                      ; R6_w=1
128: (67) r2 <<= 32                   ; R2_w=scalar(smax=9223372032559808512,umax=18446744069414584320,var_off=(0x0; 0xffffffff00000000),s32_min=0,s32_max=0,u32_max=0)
129: (c7) r2 s>>= 32                  ; R2_w=scalar(smin=-2147483648,smax=2147483647)
; if (!gen_almost_empty(lrugen, min_seq)) {
130: (65) if r2 s> 0x4 goto pc+80     ; R2_w=scalar(smin=-2147483648,smax=4)
; WRITE_ONCE(lrugen->min_seq, lrugen->min_seq + 1);
131: (79) r1 = *(u64 *)(r8 +16)       ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
132: (07) r1 += 1                     ; R1_w=scalar()
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
133: (7b) *(u64 *)(r8 +16) = r1       ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
134: (79) r1 = *(u64 *)(r8 +136)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
135: (b7) r2 = 0                      ; R2_w=0
136: (db) r2 = atomic64_fetch_or((u64 *)(r8 +56), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
137: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
138: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
139: (7b) *(u64 *)(r8 +136) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
140: (79) r1 = *(u64 *)(r8 +168)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
141: (b7) r2 = 0                      ; R2_w=0
142: (db) r2 = atomic64_fetch_or((u64 *)(r8 +24), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
143: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
144: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
145: (7b) *(u64 *)(r8 +168) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
146: (b7) r1 = 0                      ; R1_w=0
147: (db) r1 = atomic64_xchg((u64 *)(r8 +56), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
148: (b7) r1 = 0                      ; R1_w=0
149: (db) r1 = atomic64_xchg((u64 *)(r8 +24), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
150: (79) r1 = *(u64 *)(r8 +144)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
151: (b7) r2 = 0                      ; R2_w=0
152: (db) r2 = atomic64_fetch_or((u64 *)(r8 +64), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
153: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
154: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
155: (7b) *(u64 *)(r8 +144) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
156: (79) r1 = *(u64 *)(r8 +176)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
157: (b7) r2 = 0                      ; R2_w=0
158: (db) r2 = atomic64_fetch_or((u64 *)(r8 +32), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
159: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; sum += lrugen->protected[tier - 1];
160: (79) r1 = *(u64 *)(r8 +200)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum += lrugen->protected[tier - 1];
161: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
162: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
163: (7b) *(u64 *)(r8 +176) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
164: (b7) r1 = 0                      ; R1_w=0
165: (db) r1 = atomic64_xchg((u64 *)(r8 +64), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
166: (b7) r1 = 0                      ; R1_w=0
167: (db) r1 = atomic64_xchg((u64 *)(r8 +32), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
168: (7b) *(u64 *)(r8 +200) = r3      ; R3=0 R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
169: (79) r1 = *(u64 *)(r8 +152)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
170: (b7) r2 = 0                      ; R2_w=0
171: (db) r2 = atomic64_fetch_or((u64 *)(r8 +72), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
172: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
173: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
174: (7b) *(u64 *)(r8 +152) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
175: (79) r1 = *(u64 *)(r8 +184)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
176: (b7) r2 = 0                      ; R2_w=0
177: (db) r2 = atomic64_fetch_or((u64 *)(r8 +40), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
178: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; sum += lrugen->protected[tier - 1];
179: (79) r1 = *(u64 *)(r8 +208)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum += lrugen->protected[tier - 1];
180: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
181: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
182: (7b) *(u64 *)(r8 +184) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
183: (b7) r1 = 0                      ; R1_w=0
184: (db) r1 = atomic64_xchg((u64 *)(r8 +72), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
185: (b7) r1 = 0                      ; R1_w=0
186: (db) r1 = atomic64_xchg((u64 *)(r8 +40), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
187: (7b) *(u64 *)(r8 +208) = r3      ; R3=0 R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
188: (79) r1 = *(u64 *)(r8 +160)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
189: (b7) r2 = 0                      ; R2_w=0
190: (db) r2 = atomic64_fetch_or((u64 *)(r8 +80), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
191: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
192: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
193: (7b) *(u64 *)(r8 +160) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
194: (79) r1 = *(u64 *)(r8 +192)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
195: (b7) r2 = 0                      ; R2_w=0
196: (db) r2 = atomic64_fetch_or((u64 *)(r8 +48), r2)         ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
197: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; sum += lrugen->protected[tier - 1];
198: (79) r1 = *(u64 *)(r8 +216)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum += lrugen->protected[tier - 1];
199: (0f) r2 += r1                    ; R1_w=scalar() R2_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
200: (77) r2 >>= 1                    ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
201: (7b) *(u64 *)(r8 +192) = r2      ; R2_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
202: (b7) r1 = 0                      ; R1_w=0
203: (db) r1 = atomic64_xchg((u64 *)(r8 +80), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
204: (b7) r1 = 0                      ; R1_w=0
205: (db) r1 = atomic64_xchg((u64 *)(r8 +48), r1)     ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
206: (7b) *(u64 *)(r8 +216) = r3      ; R3=0 R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; WRITE_ONCE(lrugen->max_seq, lrugen->max_seq + 1);
207: (79) r1 = *(u64 *)(r8 +8)        ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
208: (07) r1 += 1                     ; R1_w=scalar()
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
209: (7b) *(u64 *)(r8 +8) = r1        ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
210: (b7) r6 = 0                      ; R6_w=0
; if (max_seq - min_seq > MIN_NR_GENS)
211: (1f) r4 -= r5                    ; R4_w=scalar() R5=scalar()
212: (b7) r1 = 3                      ; R1_w=3
; if (max_seq - min_seq > MIN_NR_GENS)
213: (2d) if r1 > r4 goto pc+91       ; R1_w=3 R4_w=scalar(umin=3)
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
214: (79) r1 = *(u64 *)(r8 +16)       ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; return seq % MAX_NR_GENS;
215: (57) r1 &= 3                     ; R1_w=scalar(umax=3,var_off=(0x0; 0x3))
; return max(0, atomic_long_read(&lrugen->nr_pages[gen_idx]));
216: (67) r1 <<= 3                    ; R1_w=scalar(umax=24,var_off=(0x0; 0x18))
217: (bf) r2 = r8                     ; R2_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
218: (0f) r2 += r1                    ; R1_w=scalar(umax=24,var_off=(0x0; 0x18)) R2_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
219: (b7) r1 = 0                      ; R1_w=0
220: (b7) r3 = 0                      ; R3_w=0
221: (db) r3 = atomic64_fetch_or((u64 *)(r2 +224), r3)        ; R2_w=map_value(id=1,off=0,ks=4,vs=256,umax=24,var_off=(0x0; 0x18)) R3_w=scalar()
222: (6d) if r1 s> r3 goto pc+6       ; R1_w=0 R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
223: (07) r2 += 224                   ; R2_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18))
224: (b7) r3 = 0                      ; R3_w=0
225: (db) r3 = atomic64_fetch_or((u64 *)(r2 +0), r3)          ; R2_w=map_value(id=1,off=224,ks=4,vs=256,umax=24,var_off=(0x0; 0x18)) R3_w=scalar()
226: (67) r3 <<= 32                   ; R3_w=scalar(smax=9223372032559808512,umax=18446744069414584320,var_off=(0x0; 0xffffffff00000000),s32_min=0,s32_max=0,u32_max=0)
227: (c7) r3 s>>= 32                  ; R3=scalar(smin=-2147483648,smax=2147483647)
; if (!gen_almost_empty(lrugen, min_seq)) {
228: (65) if r3 s> 0x4 goto pc+76     ; R3=scalar(smin=-2147483648,smax=4)
; WRITE_ONCE(lrugen->min_seq, lrugen->min_seq + 1);
229: (79) r2 = *(u64 *)(r8 +16)       ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
230: (07) r2 += 1                     ; R2_w=scalar()
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
231: (7b) *(u64 *)(r8 +16) = r2       ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
232: (79) r2 = *(u64 *)(r8 +136)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
233: (b7) r3 = 0                      ; R3_w=0
234: (db) r3 = atomic64_fetch_or((u64 *)(r8 +56), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
235: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
236: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
237: (7b) *(u64 *)(r8 +136) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
238: (79) r2 = *(u64 *)(r8 +168)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
239: (b7) r3 = 0                      ; R3_w=0
240: (db) r3 = atomic64_fetch_or((u64 *)(r8 +24), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
241: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
242: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
243: (7b) *(u64 *)(r8 +168) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
244: (b7) r2 = 0                      ; R2_w=0
245: (db) r2 = atomic64_xchg((u64 *)(r8 +56), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
246: (b7) r2 = 0                      ; R2_w=0
247: (db) r2 = atomic64_xchg((u64 *)(r8 +24), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
248: (79) r2 = *(u64 *)(r8 +144)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
249: (b7) r3 = 0                      ; R3_w=0
250: (db) r3 = atomic64_fetch_or((u64 *)(r8 +64), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
251: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
252: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
253: (7b) *(u64 *)(r8 +144) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
254: (79) r2 = *(u64 *)(r8 +176)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
255: (b7) r3 = 0                      ; R3_w=0
256: (db) r3 = atomic64_fetch_or((u64 *)(r8 +32), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
257: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; sum += lrugen->protected[tier - 1];
258: (79) r2 = *(u64 *)(r8 +200)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum += lrugen->protected[tier - 1];
259: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
260: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
261: (7b) *(u64 *)(r8 +176) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
262: (b7) r2 = 0                      ; R2_w=0
263: (db) r2 = atomic64_xchg((u64 *)(r8 +64), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
264: (b7) r2 = 0                      ; R2_w=0
265: (db) r2 = atomic64_xchg((u64 *)(r8 +32), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
266: (7b) *(u64 *)(r8 +200) = r1      ; R1=0 R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
267: (79) r2 = *(u64 *)(r8 +152)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
268: (b7) r3 = 0                      ; R3_w=0
269: (db) r3 = atomic64_fetch_or((u64 *)(r8 +72), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
270: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
271: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
272: (7b) *(u64 *)(r8 +152) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
273: (79) r2 = *(u64 *)(r8 +184)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
274: (b7) r3 = 0                      ; R3_w=0
275: (db) r3 = atomic64_fetch_or((u64 *)(r8 +40), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
276: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; sum += lrugen->protected[tier - 1];
277: (79) r2 = *(u64 *)(r8 +208)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum += lrugen->protected[tier - 1];
278: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
279: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
280: (7b) *(u64 *)(r8 +184) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
281: (b7) r2 = 0                      ; R2_w=0
282: (db) r2 = atomic64_xchg((u64 *)(r8 +72), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
283: (b7) r2 = 0                      ; R2_w=0
284: (db) r2 = atomic64_xchg((u64 *)(r8 +40), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
285: (7b) *(u64 *)(r8 +208) = r1      ; R1=0 R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
286: (79) r2 = *(u64 *)(r8 +160)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->refaulted[tier]);
287: (b7) r3 = 0                      ; R3_w=0
288: (db) r3 = atomic64_fetch_or((u64 *)(r8 +80), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_refaulted[tier] +
289: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_refaulted[tier], sum / 2);
290: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
291: (7b) *(u64 *)(r8 +160) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
292: (79) r2 = *(u64 *)(r8 +192)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_read(&lrugen->evicted[tier]);
293: (b7) r3 = 0                      ; R3_w=0
294: (db) r3 = atomic64_fetch_or((u64 *)(r8 +48), r3)         ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum = lrugen->avg_total[tier] +
295: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; sum += lrugen->protected[tier - 1];
296: (79) r2 = *(u64 *)(r8 +216)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; sum += lrugen->protected[tier - 1];
297: (0f) r3 += r2                    ; R2_w=scalar() R3_w=scalar()
; WRITE_ONCE(lrugen->avg_total[tier], sum / 2);
298: (77) r3 >>= 1                    ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff))
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
299: (7b) *(u64 *)(r8 +192) = r3      ; R3_w=scalar(umax=9223372036854775807,var_off=(0x0; 0x7fffffffffffffff)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->refaulted[tier]);
300: (b7) r2 = 0                      ; R2_w=0
301: (db) r2 = atomic64_xchg((u64 *)(r8 +80), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; atomic_long_zero(&lrugen->evicted[tier]);
302: (b7) r2 = 0                      ; R2_w=0
303: (db) r2 = atomic64_xchg((u64 *)(r8 +48), r2)     ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
304: (7b) *(u64 *)(r8 +216) = r1      ; R1=0 R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
305: (79) r7 = *(u64 *)(r8 +16)       ; R7_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
306: (79) r1 = *(u64 *)(r8 +8)        ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; volatile unsigned int next_gen = (oldest_gen + 1) % MAX_NR_GENS;
307: (bf) r1 = r7                     ; R1_w=scalar(id=13) R7_w=scalar(id=13)
308: (07) r1 += 1                     ; R1_w=scalar()
309: (57) r1 &= 3                     ; R1_w=scalar(umax=3,var_off=(0x0; 0x3))
; volatile unsigned int next_gen = (oldest_gen + 1) % MAX_NR_GENS;
310: (63) *(u32 *)(r10 -116) = r1     ; R1_w=scalar(umax=3,var_off=(0x0; 0x3)) R10=fp0 fp-120=mmmm????
; bpf_spin_unlock(&lrugen->lock);
311: (bf) r1 = r8                     ; R1_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
312: (85) call bpf_spin_unlock#94     ;
; if (inc_max_seq_failed) {
313: (15) if r6 == 0x0 goto pc+4      ; R6=0
;
318: (57) r7 &= 3                     ; R7_w=scalar(umax=3,var_off=(0x0; 0x3))
319: (7b) *(u64 *)(r10 -248) = r7     ; R7_w=scalar(umax=3,var_off=(0x0; 0x3)) R10=fp0 fp-248_w=
; pos->refaulted = lrugen->avg_refaulted[tier] +
320: (79) r1 = *(u64 *)(r8 +136)      ; R1_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
321: (b7) r3 = 0                      ; R3_w=0
; __sync_fetch_and_add(&lrugen->refaulted[tier], 0);
322: (b7) r0 = 0                      ; R0_w=0
323: (db) r0 = atomic64_fetch_or((u64 *)(r8 +56), r0)         ; R0_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total = lrugen->avg_total[tier] +
324: (79) r2 = *(u64 *)(r8 +168)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; __sync_fetch_and_add(&lrugen->evicted[tier], 0);
325: (b7) r7 = 0                      ; R7_w=0
326: (db) r7 = atomic64_fetch_or((u64 *)(r8 +24), r7)         ; R7_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->refaulted = lrugen->avg_refaulted[tier] +
327: (79) r4 = *(u64 *)(r10 -216)     ; R4_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
328: (79) r8 = *(u64 *)(r4 +144)      ; R4_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R8_w=scalar()
; __sync_fetch_and_add(&lrugen->refaulted[tier], 0);
329: (b7) r4 = 0                      ; R4_w=0
330: (79) r5 = *(u64 *)(r10 -216)     ; R5_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
331: (db) r4 = atomic64_fetch_or((u64 *)(r5 +64), r4)         ; R4_w=scalar() R5_w=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total = lrugen->avg_total[tier] +
332: (79) r5 = *(u64 *)(r10 -216)     ; R5_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
333: (79) r6 = *(u64 *)(r5 +176)      ; R5_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R6_w=scalar()
; __sync_fetch_and_add(&lrugen->evicted[tier], 0);
334: (b7) r5 = 0                      ; R5_w=0
335: (79) r9 = *(u64 *)(r10 -216)     ; R9_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
336: (db) r5 = atomic64_fetch_or((u64 *)(r9 +32), r5)         ; R5_w=scalar() R9_w=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->refaulted = lrugen->avg_refaulted[tier] +
337: (0f) r4 += r8                    ; R4_w=scalar() R8_w=scalar()
338: (79) r8 = *(u64 *)(r10 -216)     ; R8_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R10=fp0 fp-216=map_value
339: (0f) r2 += r7                    ; R2_w=scalar() R7_w=scalar()
340: (0f) r1 += r0                    ; R0_w=scalar() R1_w=scalar()
341: (67) r1 <<= 1                    ; R1_w=scalar(smax=9223372036854775806,umax=18446744073709551614,var_off=(0x0; 0xfffffffffffffffe),s32_max=2147483646,u32_max=-2)
342: (07) r1 += 2                     ; R1_w=scalar(smax=9223372036854775806,umax=18446744073709551614,var_off=(0x0; 0xfffffffffffffffe),s32_max=2147483646,u32_max=-2)
343: (07) r2 += 64                    ; R2_w=scalar()
344: (b7) r0 = 64                     ; R0_w=64
; return pv->refaulted < MIN_LRU_BATCH ||
345: (2d) if r0 > r4 goto pc+7        ; R0_w=64 R4_w=scalar(umin=64)
; pos->total = lrugen->avg_total[tier] +
346: (0f) r5 += r6                    ; R5_w=scalar() R6_w=scalar()
; pv->refaulted * (sp->total + MIN_LRU_BATCH) * sp->gain <=
347: (2f) r4 *= r2                    ; R2_w=scalar() R4_w=scalar()
; pos->total += lrugen->protected[tier - 1];
348: (79) r6 = *(u64 *)(r8 +200)      ; R6_w=scalar() R8_w=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total += lrugen->protected[tier - 1];
349: (0f) r5 += r6                    ; R5_w=scalar() R6_w=scalar()
; (sp->refaulted + 1) * pv->total * pv->gain;
350: (2f) r5 *= r1                    ; R1_w=scalar(smax=9223372036854775806,umax=18446744073709551614,var_off=(0x0; 0xfffffffffffffffe),s32_max=2147483646,u32_max=-2) R5_w=scalar()
351: (b7) r9 = 0                      ; R9=0
; if (!positive_ctrl_err(&sp, &pv))
352: (2d) if r4 > r5 goto pc+35       ; R4=scalar() R5=scalar()
; pos->refaulted = lrugen->avg_refaulted[tier] +
353: (79) r7 = *(u64 *)(r8 +152)      ; R7_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
354: (b7) r4 = 0                      ; R4_w=0
; __sync_fetch_and_add(&lrugen->refaulted[tier], 0);
355: (b7) r5 = 0                      ; R5_w=0
356: (db) r5 = atomic64_fetch_or((u64 *)(r8 +72), r5)         ; R5_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total = lrugen->avg_total[tier] +
357: (79) r3 = *(u64 *)(r8 +184)      ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; __sync_fetch_and_add(&lrugen->evicted[tier], 0);
358: (b7) r6 = 0                      ; R6_w=0
359: (db) r6 = atomic64_fetch_or((u64 *)(r8 +40), r6)         ; R6_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->refaulted = lrugen->avg_refaulted[tier] +
360: (0f) r5 += r7                    ; R5_w=scalar() R7_w=scalar()
; return pv->refaulted < MIN_LRU_BATCH ||
361: (2d) if r0 > r5 goto pc+8        ; R0=64 R5_w=scalar(umin=64)
; pos->total = lrugen->avg_total[tier] +
362: (0f) r6 += r3                    ; R3_w=scalar() R6_w=scalar()
; pv->refaulted * (sp->total + MIN_LRU_BATCH) * sp->gain <=
363: (2f) r5 *= r2                    ; R2=scalar() R5_w=scalar()
; pos->total += lrugen->protected[tier - 1];
364: (79) r3 = *(u64 *)(r8 +208)      ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total += lrugen->protected[tier - 1];
365: (0f) r6 += r3                    ; R3_w=scalar() R6_w=scalar()
; (sp->refaulted + 1) * pv->total * pv->gain;
366: (2f) r6 *= r1                    ; R1=scalar(smax=9223372036854775806,umax=18446744073709551614,var_off=(0x0; 0xfffffffffffffffe),s32_max=2147483646,u32_max=-2) R6_w=scalar()
367: (b7) r3 = 1                      ; R3_w=1
368: (b7) r9 = 1                      ; R9=1
; if (!positive_ctrl_err(&sp, &pv))
369: (2d) if r5 > r6 goto pc+18       ; R5=scalar() R6=scalar()
; pos->refaulted = lrugen->avg_refaulted[tier] +
370: (79) r0 = *(u64 *)(r8 +160)      ; R0_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; __sync_fetch_and_add(&lrugen->refaulted[tier], 0);
371: (b7) r5 = 0                      ; R5_w=0
372: (db) r5 = atomic64_fetch_or((u64 *)(r8 +80), r5)         ; R5_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total = lrugen->avg_total[tier] +
373: (79) r3 = *(u64 *)(r8 +192)      ; R3_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; __sync_fetch_and_add(&lrugen->evicted[tier], 0);
374: (db) r4 = atomic64_fetch_or((u64 *)(r8 +48), r4)         ; R4_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->refaulted = lrugen->avg_refaulted[tier] +
375: (0f) r5 += r0                    ; R0_w=scalar() R5_w=scalar()
376: (b7) r0 = 64                     ; R0_w=64
; return pv->refaulted < MIN_LRU_BATCH ||
377: (2d) if r0 > r5 goto pc+8        ; R0_w=64 R5_w=scalar(umin=64)
; pos->total = lrugen->avg_total[tier] +
378: (0f) r4 += r3                    ; R3_w=scalar() R4_w=scalar()
; pv->refaulted * (sp->total + MIN_LRU_BATCH) * sp->gain <=
379: (2f) r5 *= r2                    ; R2=scalar() R5_w=scalar()
; pos->total += lrugen->protected[tier - 1];
380: (79) r2 = *(u64 *)(r8 +216)      ; R2_w=scalar() R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
; pos->total += lrugen->protected[tier - 1];
381: (0f) r4 += r2                    ; R2_w=scalar() R4_w=scalar()
; (sp->refaulted + 1) * pv->total * pv->gain;
382: (2f) r4 *= r1                    ; R1=scalar(smax=9223372036854775806,umax=18446744073709551614,var_off=(0x0; 0xfffffffffffffffe),s32_max=2147483646,u32_max=-2) R4_w=scalar()
383: (b7) r9 = 2                      ; R9_w=2
384: (b7) r3 = 1                      ; R3=1
; if (!positive_ctrl_err(&sp, &pv))
385: (2d) if r5 > r4 goto pc+2        ; R4=scalar() R5=scalar()
386: (b7) r9 = 3                      ; R9_w=3
387: (b7) r3 = 1                      ; R3_w=1
; __sync_fetch_and_add(&lrugen->tier_selected[tier_idx], delta);
388: (bf) r1 = r9                     ; R1_w=3 R9_w=3
389: (67) r1 <<= 3                    ; R1_w=24
390: (bf) r2 = r8                     ; R2_w=map_value(id=1,off=0,ks=4,vs=256,imm=0) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0)
391: (0f) r2 += r1                    ; R1_w=24 R2_w=map_value(id=1,off=24,ks=4,vs=256,imm=0)
392: (b7) r1 = 1                      ; R1_w=1
; __sync_fetch_and_add(&lrugen->tier_selected[tier_idx], delta);
393: (db) lock *(u64 *)(r2 +88) += r1         ; R1_w=1 R2_w=map_value(id=1,off=24,ks=4,vs=256,imm=0)
394: (79) r7 = *(u64 *)(r10 -248)     ; R7_w=scalar(umax=3,var_off=(0x0; 0x3)) R10=fp0 fp-248=
; struct eviction_metadata ev_meta = {
395: (7b) *(u64 *)(r10 -152) = r7     ; R7_w=scalar(umax=3,var_off=(0x0; 0x3)) R10=fp0 fp-152_w=
; int candidate_tier = tier_threshold > 0 ? tier_threshold - 1 : 0;
396: (bf) r1 = r9                     ; R1_w=3 R9_w=3
397: (07) r1 += -1                    ; R1_w=2
; .tier_threshold = candidate_tier,
398: (67) r1 <<= 32                   ; R1_w=8589934592
399: (77) r1 >>= 32                   ; R1_w=2
400: (b7) r2 = 0                      ; R2_w=0
401: (55) if r3 != 0x0 goto pc+1      ; R3_w=1
; struct eviction_metadata ev_meta = {
403: (7b) *(u64 *)(r10 -128) = r1     ; R1=2 R10=fp0 fp-128_w=2
; .next_gen = next_gen,
404: (61) r1 = *(u32 *)(r10 -116)     ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R10=fp0 fp-120=mmmm????
; struct eviction_metadata ev_meta = {
405: (7b) *(u64 *)(r10 -144) = r1     ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R10=fp0 fp-144_w=
406: (7b) *(u64 *)(r10 -136) = r2     ; R2=0 R10=fp0 fp-136_w=00000000
; u32 key = 0;
407: (63) *(u32 *)(r10 -104) = r2     ; R2=0 R10=fp0 fp-104=????0000
408: (bf) r2 = r10                    ; R2_w=fp0 R10=fp0
; struct eviction_metadata ev_meta = {
409: (07) r2 += -104                  ; R2_w=fp-104
410: (bf) r3 = r10                    ; R3_w=fp0 R10=fp0
411: (07) r3 += -152                  ; R3_w=fp-152
; int ret = bpf_map_update_elem(&mglru_percpu_array, &key, eviction_meta, BPF_ANY);
412: (18) r1 = 0xffff888105717400     ; R1_w=map_ptr(off=0,ks=4,vs=32,imm=0)
414: (b7) r4 = 0                      ; R4_w=0
415: (85) call bpf_map_update_elem#2          ; R0_w=scalar()
416: (18) r1 = 0x80000000             ; R1_w=2147483648
; if (ret < 0) {
418: (5f) r0 &= r1                    ; R0_w=scalar(umax=2147483648,var_off=(0x0; 0x80000000),s32_max=0) R1_w=2147483648
; if (ret < 0) {
419: (15) if r0 == 0x0 goto pc+4      ; R0_w=scalar(umax=2147483648,var_off=(0x0; 0x80000000),s32_max=0)
; bpf_printk("cache_ext: Failed to update eviction metadata\n");
420: (18) r1 = 0xffffc90000172088     ; R1_w=map_value(off=136,ks=4,vs=1478,imm=0)
422: (b7) r2 = 47                     ; R2_w=47
423: (85) call bpf_trace_printk#6     ; R0=scalar()
; assert_valid_gen_0(next_gen);
424: (61) r1 = *(u32 *)(r10 -116)     ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R10=fp0 fp-120=mmmm????
425: (61) r1 = *(u32 *)(r10 -116)     ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R10=fp0 fp-120=mmmm????
426: (b7) r2 = 4                      ; R2_w=4
; assert_valid_gen_0(next_gen);
427: (2d) if r2 > r1 goto pc+6 434: R0=scalar() R1_w=scalar(umax=3,var_off=(0x0; 0x3)) R2_w=4 R6=scalar() R7=scalar(umax=3,var_off=(0x0; 0x3)) R8=map_value(id=1,off=0,ks=4,vs=256,imm=0) R9=3 R10=fp0 fp-104=????mmmm fp-112=????mmmm fp-120=mmmm???? fp-128=mmmmmmmm fp-136=mmmmmmmm fp-144=mmmmmmmm fp-152=mmmmmmmm fp-216=map_value fp-224=mmmmmmmm fp-232=mmmmmmmm fp-240=trusted_ptr_ fp-248= fp-256=trusted_ptr_ fp-264=mmmmmmmm fp-272=mmmmmmmm
; assert_valid_gen_0(next_gen);
434: (b7) r6 = 0                      ; R6_w=0
; u32 cand_key = 0;
435: (63) *(u32 *)(r10 -156) = r6     ; R6_w=0 R10=fp0 fp-160=0000????
; __u32 zero = 0;
436: (63) *(u32 *)(r10 -160) = r6     ; R6_w=0 R10=fp0 fp-160=00000000
437: (bf) r2 = r10                    ; R2_w=fp0 R10=fp0
;
438: (07) r2 += -156                  ; R2_w=fp-156
439: (bf) r3 = r10                    ; R3_w=fp0 R10=fp0
440: (07) r3 += -160                  ; R3_w=fp-160
; bpf_map_update_elem(&num_candidates_map, &cand_key, &zero, BPF_ANY);
441: (18) r1 = 0xffff888105717000     ; R1_w=map_ptr(off=0,ks=4,vs=4,imm=0)
443: (b7) r4 = 0                      ; R4_w=0
444: (85) call bpf_map_update_elem#2          ; R0=scalar()
445: (bf) r4 = r10                    ; R4_w=fp0 R10=fp0
;
446: (07) r4 += -208                  ; R4_w=fp-208
; __u64 next_gen_list = mglru_lists[next_gen];
447: (61) r1 = *(u32 *)(r10 -116)     ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R10=fp0 fp-120=mmmm????
; __u64 next_gen_list = mglru_lists[next_gen];
448: (67) r1 <<= 3                    ; R1_w=scalar(umax=34359738360,var_off=(0x0; 0x7fffffff8),s32_max=2147483640,u32_max=-8)
449: (18) r2 = 0xffff888105717d10     ; R2_w=map_value(off=0,ks=4,vs=32,imm=0)
451: (18) r3 = 0xffff888105717d10     ; R3_w=map_value(off=0,ks=4,vs=32,imm=0)
453: (0f) r3 += r1                    ; R1_w=scalar(umax=34359738360,var_off=(0x0; 0x7fffffff8),s32_max=2147483640,u32_max=-8) R3_w=map_value(off=0,ks=4,vs=32,umax=34359738360,var_off=(0x0; 0x7fffffff8),s32_max=2147483640,u32_max=-8)
454: (79) r1 = *(u64 *)(r3 +0)
R3 unbounded memory access, make sure to bounds check any such access
processed 548 insns (limit 1000000) max_states_per_insn 1 total_states 25 peak_states 25 mark_read 22
-- END PROG LOAD LOG --
libbpf: prog 'mglru_evict_folios': failed to load: -13
libbpf: failed to load object 'cache_ext_mglru_ml_bpf'
libbpf: failed to load BPF skeleton 'cache_ext_mglru_ml_bpf': -13
Failed to load BPF skeleton: Permission denied
^CTraceback (most recent call last):
  File "/home/vagrant/cache_ext_lc/lc-bench/bench_mglru_ml.py", line 133, in <module>
    main()
  File "/home/vagrant/cache_ext_lc/lc-bench/bench_mglru_ml.py", line 127, in main
    mglru_lc_bench.benchmark()
  File "/home/vagrant/cache_ext_lc/lc-bench/bench_lib_ml.py", line 595, in benchmark
    self.before_benchmark(config)
  File "/home/vagrant/cache_ext_lc/lc-bench/bench_mglru_ml.py", line 84, in before_benchmark
    self.cache_ext_policy.start()
  File "/home/vagrant/cache_ext_lc/lc-bench/bench_lib_ml.py", line 60, in start
    sleep(10)
KeyboardInterrupt
