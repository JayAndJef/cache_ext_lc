#!/usr/bin/env python3
"""
Read and parse binary log files generated by cache_ext_mglru_lc.

Binary format for cache_access events (88 bytes with padding):
    uint64_t timestamp;        // 8 bytes
    uint64_t page_time_delta;  // 8 bytes
    uint64_t page_time_delta2; // 8 bytes
    uint64_t inode_time_delta; // 8 bytes
    uint64_t inode_time_delta2;// 8 bytes
    uint32_t major;            // 4 bytes
    uint32_t minor;            // 4 bytes
    uint64_t ino;              // 8 bytes
    uint64_t offset;           // 8 bytes
    uint32_t seq_distance;     // 4 bytes
    (4 bytes padding for alignment)
    uint64_t file_size;        // 8 bytes
    uint32_t frequency;        // 4 bytes
    uint32_t inode_hotness_ema;// 4 bytes

Binary format for cache_insertion events (32 bytes):
    uint64_t timestamp;      // 8 bytes
    uint32_t major;          // 4 bytes
    uint32_t minor;          // 4 bytes
    uint64_t ino;            // 8 bytes
    uint64_t index;          // 8 bytes
"""

import struct
import sys
import argparse
import csv
from pathlib import Path


class CacheAccessEvent:
    # C struct has padding after seq_distance to align file_size to 8-byte boundary
    # Layout: timestamp(8) page_time_delta(8) page_time_delta2(8) inode_time_delta(8)
    #         inode_time_delta2(8) major(4) minor(4) ino(8) offset(8)
    #         seq_distance(4) padding(4) file_size(8) frequency(4) inode_hotness_ema(4)
    FORMAT = '<QQQQQIIQQI4xQII'  # Little-endian, with 4 bytes padding after seq_distance
    SIZE = struct.calcsize(FORMAT)

    def __init__(self, data):
        unpacked = struct.unpack(self.FORMAT, data)
        self.timestamp = unpacked[0]
        self.page_time_delta = unpacked[1]
        self.page_time_delta2 = unpacked[2]
        self.inode_time_delta = unpacked[3]
        self.inode_time_delta2 = unpacked[4]
        self.major = unpacked[5]
        self.minor = unpacked[6]
        self.ino = unpacked[7]
        self.offset = unpacked[8]
        self.seq_distance = unpacked[9]
        self.file_size = unpacked[10]
        self.frequency = unpacked[11]
        self.inode_hotness_ema = unpacked[12]

    def to_csv_row(self):
        """Return data as a dictionary for CSV output matching parsing.ipynb format.
        Fields: ts=timestamp, pd=page_time_delta, p2=page_time_delta2, id=inode_time_delta,
        i2=inode_time_delta2, dm=major, dn=minor, in=inode, of=offset, sd=seq_distance,
        sz=size, fq=frequency, ie=inode_hotness_ema
        """
        return {
            'ts': self.timestamp,
            'pd': self.page_time_delta,
            'p2': self.page_time_delta2,
            'id': self.inode_time_delta,
            'i2': self.inode_time_delta2,
            'dm': self.major,
            'dn': self.minor,
            'in': self.ino,
            'of': self.offset,
            'sd': self.seq_distance,
            'sz': self.file_size,
            'fq': self.frequency,
            'ie': self.inode_hotness_ema
        }


class CacheInsertionEvent:
    FORMAT = '<QIIQQ'  # Little-endian
    SIZE = struct.calcsize(FORMAT)

    def __init__(self, data):
        unpacked = struct.unpack(self.FORMAT, data)
        self.timestamp = unpacked[0]
        self.major = unpacked[1]
        self.minor = unpacked[2]
        self.ino = unpacked[3]
        self.index = unpacked[4]

    def to_csv_row(self):
        """Return data as a dictionary for CSV output."""
        return {
            't': self.timestamp,
            'd': f"{self.major}:{self.minor}",
            'i': self.ino,
            'x': self.index
        }


def read_access_log(filepath, limit=None):
    """Read and parse cache access log file and output as CSV."""
    count = 0
    fieldnames = ['ts', 'pd', 'p2', 'id', 'i2', 'dm', 'dn', 'in', 'of', 'sd', 'sz', 'fq', 'ie']
    writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
    writer.writeheader()

    with open(filepath, 'rb') as f:
        while True:
            data = f.read(CacheAccessEvent.SIZE)
            if not data:
                break
            if len(data) < CacheAccessEvent.SIZE:
                print(f"Warning: Incomplete record at end of file ({len(data)} bytes)",
                      file=sys.stderr)
                break

            event = CacheAccessEvent(data)
            writer.writerow(event.to_csv_row())
            count += 1

            if limit and count >= limit:
                break

    return count


def read_insertion_log(filepath, limit=None):
    """Read and parse cache insertion log file and output as CSV."""
    count = 0
    fieldnames = ['t', 'd', 'i', 'x']
    writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
    writer.writeheader()

    with open(filepath, 'rb') as f:
        while True:
            data = f.read(CacheInsertionEvent.SIZE)
            if not data:
                break
            if len(data) < CacheInsertionEvent.SIZE:
                print(f"Warning: Incomplete record at end of file ({len(data)} bytes)",
                      file=sys.stderr)
                break

            event = CacheInsertionEvent(data)
            writer.writerow(event.to_csv_row())
            count += 1

            if limit and count >= limit:
                break

    return count


def main():
    parser = argparse.ArgumentParser(description='Read binary cache trace logs and output CSV')
    parser.add_argument('logfile', help='Path to binary log file')
    parser.add_argument('--type', choices=['access', 'insertion'],
                        help='Type of log file (auto-detected from filename if not specified)')
    parser.add_argument('--limit', type=int, help='Maximum number of records to read')
    parser.add_argument('--stats-only', action='store_true',
                        help='Only show statistics, not individual records')

    args = parser.parse_args()

    # Auto-detect type from filename
    log_type = args.type
    if not log_type:
        if 'access' in Path(args.logfile).name:
            log_type = 'access'
        elif 'insertion' in Path(args.logfile).name:
            log_type = 'insertion'
        else:
            print("Error: Cannot auto-detect log type. Please specify --type",
                  file=sys.stderr)
            sys.exit(1)

    # Read the log
    if args.stats_only:
        # Just count records without printing
        import os
        file_size = os.path.getsize(args.logfile)
        if log_type == 'access':
            count = file_size // CacheAccessEvent.SIZE
        else:
            count = file_size // CacheInsertionEvent.SIZE
        print(f"Total records: {count}")
    else:
        if log_type == 'access':
            count = read_access_log(args.logfile, args.limit)
        else:
            count = read_insertion_log(args.logfile, args.limit)

        print(f"\nTotal records read: {count}", file=sys.stderr)


if __name__ == '__main__':
    main()
